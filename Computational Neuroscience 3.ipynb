{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Decoding*: how well can we learn what the stimulus is by looking at neural responses?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In signal detection theory, we have distributions $p(r|-)$ and $p(r|+)$ for downwards and upwards moving stimuli, where $r$ is the number of spikes in a single trial. The two distributions overlap. Decoding means that if we see some value of $r$, we would like to say if the stimulus was upwards or downwards moving. We need to set a threshold $z$.\n",
      "\n",
      "We also want to take into account the priors $p(+)$ and $p(-)$, and scale the respective distributions. After all, the probability that there is a tiger is very small, isn't it? Thus, we get $p(r|+)p(+)$ and $p(r|-)p(-)$ for the distributions.\n",
      "\n",
      "Now let's assume there is a cost or penalty for getting it wrong. We get eaten by the tiger, or the shopping bag explodes. The goal is to cut our losses $\\text{Loss}_- = L_-p(+|r)$ and $\\text{Loss}_+ = L_+p(-|r)$. Cut your losses: answer \u201c$+$\u201d when $\\text{Loss}_+ < \\text{Loss}_-$. Using Bayes rule, we get:\n",
      "\n",
      "$$ L_+\\frac{p(r|-)p(-)}{p(r)} < L_-\\frac{p(r|+)p(+)}{p(r)} \\implies \\frac{p(r|+)}{p(r|-)} > \\frac{L_+p(-)}{L_-p(+)} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>\n",
      "\n",
      "In general, for a <font color='teal'>conditional distribution (likelihood)</font>, <font color='darkviolet'>prior distribution</font>, <font color='mediumblue'>marginal distribution</font> and <font color='darkred'>*a posteriori* distribution</font>, we have:\n",
      "\n",
      "$$\\color{darkred}{p(s|r)} = \\frac{\\color{teal}{p(r|s)}\\color{darkviolet}{p(s)}}{\\color{mediumblue}{p(r)}},$$\n",
      "\n",
      "where $p(r) = \\int ds~p(r|s)$. We have two important general decoding strategies.\n",
      "\n",
      "* The first is **maximum likelihood** (ML): find a special stimulus value $s^*$ that maximizes $\\color{teal}{p(r|s)}$.\n",
      "* The second is **maximum *a posteriori*** (MAP): find $s^*$ that maximizes $\\color{darkred}{p(s|r)}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example! Assume we have a population of neurons. Assume they are firing independently. Assume Poisson firing. For *a*-th neuron, the response curve is a function of the stimulus, $f_a(s)$. With Gaussian tuning curves, we have:\n",
      "\n",
      "$$ f_a(s) = r_\\text{max} \\exp \\left( -\\frac{1}{2} \\left[ \\frac{(s-s_a)}{\\sigma_a} \\right]^2 \\right) $$\n",
      "\n",
      "We are also assuming good coverage, that is, $\\sum^N_{a=1} f_a(s) = \\text{const}$. Spikes are produced *randomly and independently* in each time bin with a probability given by the instantaneous rate. For the number of spikes $k$ and length of time $T$, assuming Poisson,\n",
      "\n",
      "$$ p_T(k) = (rT)^k \\exp (-rT)/k! $$\n",
      "\n",
      "Substituting the tuning curves into the Poisson distribution, for *a*-th neuron we have:\n",
      "\n",
      "$$p_T(r_a|s) = \\frac{(f_a(s)T)^{r_aT}}{(r_aT)!} \\exp (-f_a(s)T)$$\n",
      "\n",
      "Because the probabilities are assumed independent, the probability of the vector $\\vec{r} = \\{r_1,\\ldots,r_n\\}$ is going to be just the product of the individual probabilities:\n",
      "\n",
      "$$ p(\\vec{r}|s) = \\prod^n_{a=1} p(r_a|s) = \\prod^n_{a=1} \\frac{(f_a(s)T)^{r_aT}}{(r_aT)!} \\exp (-f_a(s)T)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have this, let's apply maximum likelihood. It's useful to use log likelihood instead of the likelihood directly, so let's take the log of each side of the equation above:\n",
      "\n",
      "$$ \\ln p(r|s) = \\sum^n_{a=1} r_aT \\ln (f_a(s)T) - f_a(s)T - \\ln ((r_aT)!)$$\n",
      "\n",
      "Now we need to find the $s^*$ for which this is maximized. For this, we take the derivative of this equation. We required that $\\sum^n_{a=1}f_a(s)T$ be a constant, the $\\ln ((r_aT)!)$ doesn't depend on $s$. We only have the first term in the sum left.\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial s} \\ln p(r|s) = T \\sum^n_{a=1} r_a \\frac{f'_a(s)T}{f_a(s)} = T \\sum^n_{a=1} r_a \\frac{f'(s^*)}{f(s^*)} = 0$$\n",
      "\n",
      "How do we solve it?\n",
      "\n",
      "$$ f(s) = Ae^{-\\frac{1}{2\\sigma^2} (s-s_0)^2} $$\n",
      "$$ f'(s) = A\\frac{(s-s_0)}{\\sigma^2} e^{ -\\frac{1}{2\\sigma^2} (s-s_0)^2} $$\n",
      "\n",
      "Substituting, we get:\n",
      "\n",
      "$$ \\sum^n_{a=1} r_a \\frac{s-s_a}{\\sigma_a^2} = 0 $$\n",
      "\n",
      "And so, from Gaussianity of the tuning curves, we have:\n",
      "\n",
      "$$ s^* = \\frac{ \\sum r_as_a/\\sigma_a^2 }{ \\sum r_a/\\sigma_a^2 } $$\n",
      "\n",
      "If all $\\sigma$ are the same,\n",
      "\n",
      "$$ s^* = \\frac{ \\sum r_as_a }{ \\sum r_a } $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's calculate the maximum *a posteriori* solution, maximizing $\\ln p(s|r)$ with respect to $s$. We can just use Bayes law and go through the process again:\n",
      "\n",
      "$$ \\ln p(s|r) = \\ln p(r|s) + \\ln p(s) - \\ln p(r) $$\n",
      "\n",
      "$$ s^* = \\frac{ T \\sum r_as_a/\\sigma^2_a + s_{\\text{prior}}/\\sigma^2_{\\text{prior}} }{ T \\sum r_a/\\sigma^2_a + 1/\\sigma^2_{\\text{prior}} } $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>\n",
      "\n",
      "Now let's say we want an estimator $s_{\\text{Bayes}}$. We introduce an error function, $L(s, s_{\\text{Bayes}})$; minimize error.\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial s_{\\text{Bayes}}} \\int ds~L(s, s_{\\text{Bayes}}) p(s|r) = 0 $$\n",
      "\n",
      "For least squares error, $L(s,s_{\\text{Bayes}}) = (s-s_{\\text{Bayes}})^2$.\n",
      "\n",
      "$$ \\frac{\\partial}{\\partial s_{\\text{Bayes}}} \\int ds~(s-s_{\\text{Bayes}})^2 p(s|r) = 2 \\int ds~(s-s_{\\text{Bayes}}) p(s|r) = 0 $$\n",
      "\n",
      "$$ s_{\\text{Bayes}} = \\int ds~p(s|r)s $$\n",
      "\n",
      "This expression amounts to the spike-triggered average! "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}